task_type: embedding # supervised, lda, embedding âœ…

data:
  use_sample: true # Use a sample or full dataset âœ…
  sample:
    size: 10000
    top_n_tags: 200
    output_dir: data/sampled
    seed: 42
  # Paths for full data
  train_path: data/X_text.csv
  target_path: data/Y_tags.csv
  # Paths for train/test data
  train:
    sample_path: data/sampled/train/X_train.csv # for sample
    sample_target_path: data/sampled/train/Y_train.csv # for sample
    full_path: data/train/X_train.csv # for full dataset
    full_target_path: data/train/Y_train.csv # for full dataset
  test:
    sample_path: data/sampled/test/X_test.csv # for sample
    sample_target_path: data/sampled/test/Y_test.csv # for sample
    full_path: data/test/X_test.csv # for full dataset
    full_target_path: data/test/Y_test.csv # for full dataset
  test_size: 0.2 # test sample size

mlflow:
  experiment_name: embedding SAMPLE 10K experiment ðŸŸ¢ # âœ…
  tracking_uri: sqlite:///mlflow.db
  registry_uri: sqlite:///mlflow.db
  artifacts_location: ./mlruns

# Settings for vectorizer
vectorizer:
  method: tfidf # tfidf or count âœ…
  params:
    max_features: 5000
    stop_words: english

# Settings for LDA model
lda:
  model: lda
  num_topics: 10
  passes: 10
  model_path: models/lda/lda_model.gensim
  visualization:
    output_dir: outputs/reports
    topics_vis_path: lda_visualization.html
    distribution_vis_path: document_distribution.png

# Settings for supervised models
supervised:
  model: xgboost # logreg, rf, xgboost âœ…

  models:

    logreg:
      base_params:
        solver: saga
        max_iter: 1000
        random_state: 42
        n_jobs: -1
        class_weight: "balanced"
        penalty: "l2"
        C: 1.0
      search_params:
        estimator__C: [0.1, 1.0]
        estimator__class_weight: ["balanced"]
        estimator__penalty: ["l2"]
        estimator__max_iter: [1000]

    rf:
      base_params:
        random_state: 42
        n_jobs: -1
        n_estimators: 200
        max_depth: 20
        min_samples_split: 5
        min_samples_leaf: 2
        max_features: "sqrt"
        class_weight: "balanced"
      #search_params:
        #estimator__n_estimators: [100, 200, 300]
        #estimator__max_depth: [10, 20, 30]
        #estimator__min_samples_split: [2, 5]
        #estimator__min_samples_leaf: [1, 2]
        #estimator__max_features: ["sqrt", "log2"]

    xgboost:
      base_params:
        objective: "binary:logistic"
        eval_metric: "logloss"
        random_state: 42
        n_jobs: -1
        n_estimators: 200
        max_depth: 6
        learning_rate: 0.1
        subsample: 0.8
        colsample_bytree: 0.8
        tree_method: "hist"
        scale_pos_weight: 1
      #search_params:
        #estimator__n_estimators: [100, 200, 300]
        #estimator__max_depth: [4, 6, 8]
        #estimator__learning_rate: [0.01, 0.05, 0.1]
        #estimator__subsample: [0.8, 1.0]
        #estimator__colsample_bytree: [0.8, 1.0]
        #estimator__min_child_weight: [1, 3, 5]

# Settings for embedding models
embedding:
  model: word2vec # bert, use, word2vec
  models:
    bert:
      model_name: "all-MiniLM-L6-v2"
      base_params:
        model_name: "bert-base-uncased"
        max_length: 512
    use:
      model_url: "https://tfhub.dev/google/universal-sentence-encoder/4"
      base_params:
        batch_size: 32
    word2vec:
      vector_size: 300
      window: 5
      min_count: 1
      workers: 4
      base_params:
        model_name: "word2vec-google-news-300"

  classifier: xgboost # xgboost, logreg, rf
  
  classifiers:

    xgboost:
      base_params:
        n_estimators: 100
        max_depth: 5
        learning_rate: 0.1
        random_state: 42
      search_params:
        estimator__n_estimators: [50, 100, 200]
        estimator__max_depth: [3, 5, 7]
        estimator__learning_rate: [0.01, 0.1, 0.2]
    logreg:
      base_params:
        solver: saga
        max_iter: 1000
        random_state: 42
        n_jobs: -1
        class_weight: "balanced"
      search_params:
        estimator__C: [0.1, 1.0]
        estimator__class_weight: ["balanced"]
    rf:
      base_params:
        random_state: 42
        n_jobs: -1
        n_estimators: 200
        max_depth: 20
        min_samples_split: 5
        min_samples_leaf: 2
        max_features: "sqrt"
        class_weight: "balanced"
      search_params:
        estimator__n_estimators: [100, 200, 300]
        estimator__max_depth: [10, 20, 30]
        estimator__min_samples_split: [2, 5]
        estimator__min_samples_leaf: [1, 2]
        estimator__max_features: ["sqrt", "log2"]
